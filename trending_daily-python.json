[
{"description": "An unnecessarily tiny and minimal implementation of GPT-2 in NumPy.", "language": "Python", "repo": "picoGPT", "new_stars": 93, "stars": 298, "owner": "jaymody", "forks": 19},
{"description": "Official Pytorch Implementation for \u201cPlug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation\u201d", "language": "Python", "repo": "plug-and-play", "new_stars": 62, "stars": 316, "owner": "MichalGeyer", "forks": 11},
{"description": "A collaboration friendly studio for NeRFs", "language": "Python", "repo": "nerfstudio", "new_stars": 32, "stars": 3469, "owner": "nerfstudio-project", "forks": 318},
{"description": "This is an application project of 'chatgpt',only applicable to desktop environment.", "language": "Python", "repo": "EASYChatGPT", "new_stars": 20, "stars": 284, "owner": "AIGCT", "forks": 359},
{"description": "An unofficial PyTorch implementation of the audio LM VALL-E", "language": "Python", "repo": "vall-e", "new_stars": 43, "stars": 1129, "owner": "enhuiz", "forks": 167},
{"description": "\u4f7f\u7528ChatGPT\u642d\u5efa\u5fae\u4fe1\u804a\u5929\u673a\u5668\u4eba\uff0c\u57fa\u4e8eOpenAI API\u548citchat\u5b9e\u73b0\u3002Wechat robot based on ChatGPT, which using OpenAI api and itchat library.", "language": "Python", "repo": "chatgpt-on-wechat", "new_stars": 229, "stars": 2040, "owner": "zhayujie", "forks": 375},
{"description": "Stable Diffusion web UI", "language": "Python", "repo": "stable-diffusion-webui", "new_stars": 179, "stars": 31970, "owner": "AUTOMATIC1111", "forks": 6134},
{"description": "The simplest, fastest repository for training/finetuning medium-sized GPTs.", "language": "Python", "repo": "nanoGPT", "new_stars": 121, "stars": 12484, "owner": "karpathy", "forks": 1020},
{"description": "BUG BOUNTY WRITEUPS - OWASP TOP 10", "language": "Python", "repo": "Bug_Bounty_writeups", "new_stars": 9, "stars": 705, "owner": "alexbieber", "forks": 97},
{"description": "Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.", "language": "Python", "repo": "transformers", "new_stars": 96, "stars": 80336, "owner": "huggingface", "forks": 17942},
{"description": "Building applications with LLMs through composability", "language": "Python", "repo": "langchain", "new_stars": 101, "stars": 6335, "owner": "hwchase17", "forks": 527},
{"description": "Convert any music library into a music production sample-library with ML", "language": "Python", "repo": "polymath", "new_stars": 65, "stars": 683, "owner": "samim23", "forks": 37},
{"description": null, "language": "Python", "repo": "hard-prompts-made-easy", "new_stars": 45, "stars": 224, "owner": "YuxinWenRick", "forks": 18},
{"description": "OpenMMLab YOLO series toolbox and benchmark. Implemented RTMDet, YOLOv5, YOLOv6, YOLOv7, YOLOv8,YOLOX, PPYOLOE, etc.", "language": "Python", "repo": "mmyolo", "new_stars": 5, "stars": 1194, "owner": "open-mmlab", "forks": 196},
{"description": "Reverse engineered API of Microsoft's Bing Chat", "language": "Python", "repo": "EdgeGPT", "new_stars": 197, "stars": 341, "owner": "acheong08", "forks": 33},
{"description": "Search The Deep Web Straight From Your Terminal", "language": "Python", "repo": "darkdump", "new_stars": 19, "stars": 334, "owner": "josh0xA", "forks": 77},
{"description": "YOLOv3 in PyTorch > ONNX > CoreML > TFLite", "language": "Python", "repo": "yolov3", "new_stars": 2, "stars": 9304, "owner": "ultralytics", "forks": 3379},
{"description": null, "language": "Python", "repo": "ng-video-lecture", "new_stars": 17, "stars": 1197, "owner": "karpathy", "forks": 218},
{"description": "All of the ad-hoc things you're doing to manage incidents today, done for you, and much more!", "language": "Python", "repo": "dispatch", "new_stars": 8, "stars": 3730, "owner": "Netflix", "forks": 384},
{"description": "Chinese version of GPT2 training code, using BERT tokenizer.", "language": "Python", "repo": "GPT2-Chinese", "new_stars": 18, "stars": 5855, "owner": "Morizeyao", "forks": 1385},
{"description": "A simple but complete full-attention transformer with a set of promising experimental features from various papers", "language": "Python", "repo": "x-transformers", "new_stars": 7, "stars": 2411, "owner": "lucidrains", "forks": 212},
{"description": "A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training", "language": "Python", "repo": "minGPT", "new_stars": 43, "stars": 12812, "owner": "karpathy", "forks": 1419},
{"description": "Code for the paper \"Language Models are Unsupervised Multitask Learners\"", "language": "Python", "repo": "gpt-2", "new_stars": 73, "stars": 16706, "owner": "openai", "forks": 4226},
{"description": "Fast and memory-efficient exact attention", "language": "Python", "repo": "flash-attention", "new_stars": 15, "stars": 1701, "owner": "HazyResearch", "forks": 144},
{"description": "Official repository of OFA (ICML 2022). Paper: OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework", "language": "Python", "repo": "OFA", "new_stars": 5, "stars": 1521, "owner": "OFA-Sys", "forks": 191}
]