[
{"description": "Inference code for LLaMA models", "language": "Python", "repo": "llama", "new_stars": 1480, "stars": 31696, "owner": "facebookresearch", "forks": 4965},
{"description": "Advanced Python Mastery (course by", "language": "Python", "repo": "python-mastery", "new_stars": 2274, "stars": 5469, "owner": "dabeaz-course", "forks": 611},
{"description": "Examples and recipes for Llama 2 model", "language": "Python", "repo": "llama-recipes", "new_stars": 429, "stars": 1171, "owner": "facebookresearch", "forks": 117},
{"description": null, "language": "Python", "repo": "NeuralHaircut", "new_stars": 44, "stars": 240, "owner": "SamsungLabs", "forks": 21},
{"description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities", "language": "Python", "repo": "unilm", "new_stars": 95, "stars": 14304, "owner": "microsoft", "forks": 2014},
{"description": "Open source implementation of the ChatGPT Code Interpreter", "language": "Python", "repo": "codeinterpreter-api", "new_stars": 293, "stars": 1502, "owner": "shroominic", "forks": 126},
{"description": "\u4f4e\u6210\u672c\uff0c\u9ad8\u6548\u7387\uff0c\u7b80\u5355\u5b9e\u73b0\u7684\u5e01\u5b89\u5408\u7ea6\u91cf\u5316\u7cfb\u7edf\u67b6\u6784", "language": "Python", "repo": "c-binance-future-quant", "new_stars": 21, "stars": 228, "owner": "Melelery", "forks": 157},
{"description": "\u5927\u9ea6\u7f51\u62a2\u7968\u811a\u672c", "language": "Python", "repo": "Automatic_ticket_purchase", "new_stars": 30, "stars": 2069, "owner": "MakiNaruto", "forks": 482},
{"description": "Repo for adapting Meta LlaMA2 in Chinese! META\u6700\u65b0\u53d1\u5e03\u7684LlaMA2\u7684\u6c49\u5316\u7248\uff01 \uff08\u5b8c\u5168\u5f00\u6e90\u53ef\u5546\u7528\uff09", "language": "Python", "repo": "Chinese-LlaMA2", "new_stars": 60, "stars": 300, "owner": "michael-wzhu", "forks": 10},
{"description": "LLaMA v2 Chatbot", "language": "Python", "repo": "llama2-chatbot", "new_stars": 180, "stars": 670, "owner": "a16z-infra", "forks": 92},
{"description": "A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.", "language": "Python", "repo": "text-generation-webui", "new_stars": 247, "stars": 18402, "owner": "oobabooga", "forks": 2340},
{"description": "ShortGPT - An experimental AI framework for automated short/video content creation. Enables creators to rapidly produce, manage, and deliver content using AI and automation.", "language": "Python", "repo": "ShortGPT", "new_stars": 266, "stars": 1319, "owner": "RayVentura", "forks": 116},
{"description": "Practical Python Programming (course by", "language": "Python", "repo": "practical-python", "new_stars": 84, "stars": 8891, "owner": "dabeaz-course", "forks": 5459},
{"description": "Hey there new grad", "language": "Python", "repo": "New-Grad-2024", "new_stars": 46, "stars": 2089, "owner": "ReaVNaiL", "forks": 114},
{"description": "Diagram as Code for prototyping cloud system architectures", "language": "Python", "repo": "diagrams", "new_stars": 128, "stars": 30713, "owner": "mingrammer", "forks": 1960},
{"description": "Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.", "language": "Python", "repo": "lit-llama", "new_stars": 11, "stars": 4895, "owner": "Lightning-AI", "forks": 393},
{"description": "\u4e2d\u6587LLaMA&Alpaca\u5927\u8bed\u8a00\u6a21\u578b+\u672c\u5730CPU/GPU\u8bad\u7ec3\u90e8\u7f72 (Chinese LLaMA & Alpaca LLMs)", "language": "Python", "repo": "Chinese-LLaMA-Alpaca", "new_stars": 87, "stars": 12997, "owner": "ymcui", "forks": 1369},
{"description": "Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.", "language": "Python", "repo": "transformers", "new_stars": 186, "stars": 108397, "owner": "huggingface", "forks": 21534},
{"description": "LLaMA Cog template", "language": "Python", "repo": "cog-llama-template", "new_stars": 17, "stars": 175, "owner": "a16z-infra", "forks": 13},
{"description": "Building applications with LLMs through composability", "language": "Python", "repo": "langchain", "new_stars": 189, "stars": 55639, "owner": "hwchase17", "forks": 7234},
{"description": null, "language": "Python", "repo": "webdriver_manager", "new_stars": 3, "stars": 1411, "owner": "SergeyPirogov", "forks": 323},
{"description": "Firefly(\u6d41\u8424): \u4e2d\u6587\u5bf9\u8bdd\u5f0f\u5927\u8bed\u8a00\u6a21\u578b(\u5168\u91cf\u5fae\u8c03+QLoRA)\uff0c\u652f\u6301\u5fae\u8c03Llma2\u3001Llama\u3001Baichuan\u3001InternLM\u3001Ziya\u3001Bloom\u7b49\u5927\u6a21\u578b", "language": "Python", "repo": "Firefly", "new_stars": 40, "stars": 1130, "owner": "yangjianxin1", "forks": 87},
{"description": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.", "language": "Python", "repo": "FastChat", "new_stars": 64, "stars": 25268, "owner": "lm-sys", "forks": 2969},
{"description": "Uses Various AI Service APIs to generate memes with text and images", "language": "Python", "repo": "Full-Stack-AI-Meme-Generator", "new_stars": 29, "stars": 125, "owner": "ThioJoe", "forks": 14},
{"description": "Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading", "language": "Python", "repo": "petals", "new_stars": 179, "stars": 5996, "owner": "bigscience-workshop", "forks": 259}
]