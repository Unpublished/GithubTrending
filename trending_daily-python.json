[
{"description": "Inference code for LLaMA models", "language": "Python", "repo": "llama", "new_stars": 2020, "stars": 30968, "owner": "facebookresearch", "forks": 4896},
{"description": "Advanced Python Mastery (course by", "language": "Python", "repo": "python-mastery", "new_stars": 2589, "stars": 5178, "owner": "dabeaz-course", "forks": 569},
{"description": "Examples and recipes for Llama 2 model", "language": "Python", "repo": "llama-recipes", "new_stars": 244, "stars": 1073, "owner": "facebookresearch", "forks": 111},
{"description": null, "language": "Python", "repo": "NeuralHaircut", "new_stars": 44, "stars": 198, "owner": "SamsungLabs", "forks": 17},
{"description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities", "language": "Python", "repo": "unilm", "new_stars": 162, "stars": 14269, "owner": "microsoft", "forks": 2011},
{"description": "Open source implementation of the ChatGPT Code Interpreter", "language": "Python", "repo": "codeinterpreter-api", "new_stars": 485, "stars": 1450, "owner": "shroominic", "forks": 122},
{"description": "\u4f4e\u6210\u672c\uff0c\u9ad8\u6548\u7387\uff0c\u7b80\u5355\u5b9e\u73b0\u7684\u5e01\u5b89\u5408\u7ea6\u91cf\u5316\u7cfb\u7edf\u67b6\u6784", "language": "Python", "repo": "c-binance-future-quant", "new_stars": 21, "stars": 206, "owner": "Melelery", "forks": 147},
{"description": "\u5927\u9ea6\u7f51\u62a2\u7968\u811a\u672c", "language": "Python", "repo": "Automatic_ticket_purchase", "new_stars": 30, "stars": 2048, "owner": "MakiNaruto", "forks": 482},
{"description": "Repo for adapting Meta LlaMA2 in Chinese! META\u6700\u65b0\u53d1\u5e03\u7684LlaMA2\u7684\u6c49\u5316\u7248\uff01 \uff08\u5b8c\u5168\u5f00\u6e90\u53ef\u5546\u7528\uff09", "language": "Python", "repo": "Chinese-LlaMA2", "new_stars": 60, "stars": 280, "owner": "michael-wzhu", "forks": 10},
{"description": "LLaMA v2 Chatbot", "language": "Python", "repo": "llama2-chatbot", "new_stars": 156, "stars": 638, "owner": "a16z-infra", "forks": 87},
{"description": "A gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.", "language": "Python", "repo": "text-generation-webui", "new_stars": 159, "stars": 18360, "owner": "oobabooga", "forks": 2335},
{"description": "ShortGPT - An experimental AI framework for automated short/video content creation. Enables creators to rapidly produce, manage, and deliver content using AI and automation.", "language": "Python", "repo": "ShortGPT", "new_stars": 268, "stars": 1252, "owner": "RayVentura", "forks": 106},
{"description": "Practical Python Programming (course by", "language": "Python", "repo": "practical-python", "new_stars": 56, "stars": 8867, "owner": "dabeaz-course", "forks": 5449},
{"description": "Hey there new grad", "language": "Python", "repo": "New-Grad-2024", "new_stars": 46, "stars": 2078, "owner": "ReaVNaiL", "forks": 113},
{"description": "Diagram as Code for prototyping cloud system architectures", "language": "Python", "repo": "diagrams", "new_stars": 128, "stars": 30678, "owner": "mingrammer", "forks": 1958},
{"description": "Implementation of the LLaMA language model based on nanoGPT. Supports flash attention, Int8 and GPTQ 4bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.", "language": "Python", "repo": "lit-llama", "new_stars": 11, "stars": 4893, "owner": "Lightning-AI", "forks": 393},
{"description": "\u4e2d\u6587LLaMA&Alpaca\u5927\u8bed\u8a00\u6a21\u578b+\u672c\u5730CPU/GPU\u8bad\u7ec3\u90e8\u7f72 (Chinese LLaMA & Alpaca LLMs)", "language": "Python", "repo": "Chinese-LLaMA-Alpaca", "new_stars": 122, "stars": 12985, "owner": "ymcui", "forks": 1369},
{"description": "Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.", "language": "Python", "repo": "transformers", "new_stars": 179, "stars": 108357, "owner": "huggingface", "forks": 21531},
{"description": "LLaMA Cog template", "language": "Python", "repo": "cog-llama-template", "new_stars": 17, "stars": 172, "owner": "a16z-infra", "forks": 13},
{"description": "Building applications with LLMs through composability", "language": "Python", "repo": "langchain", "new_stars": 189, "stars": 55611, "owner": "hwchase17", "forks": 7227},
{"description": null, "language": "Python", "repo": "webdriver_manager", "new_stars": 3, "stars": 1408, "owner": "SergeyPirogov", "forks": 323},
{"description": "Firefly(\u6d41\u8424): \u4e2d\u6587\u5bf9\u8bdd\u5f0f\u5927\u8bed\u8a00\u6a21\u578b(\u5168\u91cf\u5fae\u8c03+QLoRA)", "language": "Python", "repo": "Firefly", "new_stars": 40, "stars": 1127, "owner": "yangjianxin1", "forks": 87},
{"description": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and FastChat-T5.", "language": "Python", "repo": "FastChat", "new_stars": 64, "stars": 25258, "owner": "lm-sys", "forks": 2967},
{"description": "Uses Various AI Service APIs to generate memes with text and images", "language": "Python", "repo": "Full-Stack-AI-Meme-Generator", "new_stars": 29, "stars": 120, "owner": "ThioJoe", "forks": 14},
{"description": "Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading", "language": "Python", "repo": "petals", "new_stars": 247, "stars": 5949, "owner": "bigscience-workshop", "forks": 253}
]