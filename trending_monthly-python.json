[
{"description": "Inference code for LLaMA models", "language": "Python", "repo": "llama", "new_stars": 11452, "stars": 35499, "owner": "facebookresearch", "forks": 5643},
{"description": "aider is GPT powered coding in your terminal", "language": "Python", "repo": "aider", "new_stars": 2161, "stars": 2690, "owner": "paul-gauthier", "forks": 442},
{"description": "GPT 3.5/4 with a Chat Web UI. No API key required.", "language": "Python", "repo": "freegpt-webui", "new_stars": 3468, "stars": 4144, "owner": "ramonvc", "forks": 1208},
{"description": "A powerful and modular stable diffusion GUI with a graph/nodes interface.", "language": "Python", "repo": "ComfyUI", "new_stars": 2020, "stars": 8484, "owner": "comfyanonymous", "forks": 753},
{"description": "Private Q&A and summarization of documents+images or chat with local GPT, 100% private, Apache 2.0. Supports LLaMa2, llama.cpp, and more. Demo:", "language": "Python", "repo": "h2ogpt", "new_stars": 3056, "stars": 6082, "owner": "h2oai", "forks": 694},
{"description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities", "language": "Python", "repo": "unilm", "new_stars": 1847, "stars": 14701, "owner": "microsoft", "forks": 2047},
{"description": "ChatGLM2-6B: An Open Bilingual Chat LLM | \u5f00\u6e90\u53cc\u8bed\u5bf9\u8bdd\u8bed\u8a00\u6a21\u578b", "language": "Python", "repo": "ChatGLM2-6B", "new_stars": 6533, "stars": 10365, "owner": "THUDM", "forks": 1680},
{"description": "You like pytorch? You like micrograd? You love tinygrad!", "language": "Python", "repo": "tinygrad", "new_stars": 3303, "stars": 18383, "owner": "tinygrad", "forks": 2335},
{"description": null, "language": "Python", "repo": "chatgpt-retrieval", "new_stars": 446, "stars": 1004, "owner": "techleadhd", "forks": 461},
{"description": "Fast and memory-efficient exact attention", "language": "Python", "repo": "flash-attention", "new_stars": 1796, "stars": 5666, "owner": "Dao-AILab", "forks": 445},
{"description": "Generative Models by Stability AI", "language": "Python", "repo": "generative-models", "new_stars": 3212, "stars": 5556, "owner": "Stability-AI", "forks": 767},
{"description": "A high-throughput and memory-efficient inference and serving engine for LLMs", "language": "Python", "repo": "vllm", "new_stars": 1615, "stars": 4220, "owner": "vllm-project", "forks": 401},
{"description": "Fine-tuning ChatGLM-6B with PEFT | \u57fa\u4e8e PEFT \u7684\u9ad8\u6548 ChatGLM \u5fae\u8c03", "language": "Python", "repo": "ChatGLM-Efficient-Tuning", "new_stars": 1083, "stars": 2578, "owner": "hiyouga", "forks": 371},
{"description": "Voice data <= 10 mins can also be used to train a good VC model!", "language": "Python", "repo": "Retrieval-based-Voice-Conversion-WebUI", "new_stars": 2931, "stars": 8591, "owner": "RVC-Project", "forks": 1256},
{"description": "[PREVIEW] Sample code for a simple web chat experience targeting chatGPT through AOAI.", "language": "Python", "repo": "sample-app-aoai-chatGPT", "new_stars": 109, "stars": 302, "owner": "microsoft", "forks": 288},
{"description": "MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. \u8bad\u7ec3\u533b\u7597\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u5305\u62ec\u4e8c\u6b21\u9884\u8bad\u7ec3\u3001\u6709\u76d1\u7763\u5fae\u8c03\u3001\u5956\u52b1\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "language": "Python", "repo": "MedicalGPT", "new_stars": 455, "stars": 1058, "owner": "shibing624", "forks": 160},
{"description": "\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u30dc\u30a4\u30b9\u30c1\u30a7\u30f3\u30b8\u30e3\u30fc Realtime Voice Changer", "language": "Python", "repo": "voice-changer", "new_stars": 3338, "stars": 8730, "owner": "w-okada", "forks": 979},
{"description": "Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)", "language": "Python", "repo": "PaddleOCR", "new_stars": 807, "stars": 32086, "owner": "PaddlePaddle", "forks": 6515},
{"description": "Train neural networks up to 7x faster", "language": "Python", "repo": "composer", "new_stars": 802, "stars": 4441, "owner": "mosaicml", "forks": 331},
{"description": "Faster Whisper transcription with CTranslate2", "language": "Python", "repo": "faster-whisper", "new_stars": 1094, "stars": 3829, "owner": "guillaumekln", "forks": 272},
{"description": "Train transformer language models with reinforcement learning.", "language": "Python", "repo": "trl", "new_stars": 500, "stars": 4379, "owner": "lvwerra", "forks": 475},
{"description": "Hey there new grad", "language": "Python", "repo": "New-Grad-2024", "new_stars": 1183, "stars": 2299, "owner": "ReaVNaiL", "forks": 135},
{"description": "Data validation using Python type hints", "language": "Python", "repo": "pydantic", "new_stars": 731, "stars": 15017, "owner": "pydantic", "forks": 1341},
{"description": "Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading", "language": "Python", "repo": "petals", "new_stars": 1537, "stars": 6420, "owner": "bigscience-workshop", "forks": 288},
{"description": "The official GitHub page for the survey paper \"A Survey of Large Language Models\".", "language": "Python", "repo": "LLMSurvey", "new_stars": 2024, "stars": 4435, "owner": "RUCAIBox", "forks": 339}
]